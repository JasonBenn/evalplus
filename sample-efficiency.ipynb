{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc348b2-7490-4b6d-998d-761a437bcaae",
   "metadata": {},
   "source": [
    "## Estimating average sample efficiency is much faster than computing pass@k\n",
    "\n",
    "pass@10 for HumanEval+ takes about 10 minutes to compute. If we have thousands of features that we'd like to try using to steer the base model, that's too expensive.\n",
    "\n",
    "Estimating the sample efficiency of a task means testing how many rollouts it takes to get our *first* pass.\n",
    "\n",
    "Many examples are also uninteresting:\n",
    "- If they're already too easy for the base model, and it passes on the first try, then steering can't improve the base model's performance on that task.\n",
    "- If they're too hard, then it'd require a lot of samples (with an unclear upper bound) to see whether the steered base model has improved.\n",
    "- This notebook finds problems that are in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa859c65-d3a6-4f18-8c52-ecef56b4233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "results = json.loads(open(\"evalplus_results/humaneval/Qwen--Qwen2.5-Coder-1.5B-Instruct_vllm_temp_1.0.eval_results.json\").read())\n",
    "# results = json.loads(open(\"evalplus_results/mbpp/Qwen--Qwen2.5-Coder-1.5B-Instruct_vllm_temp_1.0.eval_results.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86c25daf-3329-462d-9b47-503472e22504",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_by_task_id = {}\n",
    "\n",
    "TOO_EASY_THRESHOLD = 2\n",
    "\n",
    "for task_id, samples in results['eval'].items():\n",
    "    for n, attempt in enumerate(samples):\n",
    "        n += 1  # make n_samples 1-indexed\n",
    "        if attempt['base_status'] == 'pass':\n",
    "            n_samples_by_task_id[task_id] = n\n",
    "            break\n",
    "        else:  # too hard\n",
    "            n_samples_by_task_id[task_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d6dd4b3-e700-4c64-be45-d314f3d2c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([6, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, None, 10, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, None, 3, 3, 1, 2, 1, 2, 1, 5, 1, 1, 2, 3, 5, 5, 1, 10, 1, 1, 1, 1, 1, 1, 1, 3, 3, None, 5, 1, 1, 1, 4, 1, None, 3, 2, 1, 1, None, 1, 2, None, 1, 1, 7, None, 1, 2, 1, 1, 2, 5, None, 2, None, None, 1, None, 1, None, 2, 1, 1, None, 2, 4, 2, 6, 1, 1, None, 1, 1, 1, 2, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 8, 2, 1, None])\n"
     ]
    }
   ],
   "source": [
    "print(n_samples_by_task_id.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fde0c9-4c49-43fc-ac17-d804c00d0595",
   "metadata": {},
   "source": [
    "### Tasks in the sweet spot of difficulty - maybe we test on these?\n",
    "These are tasks where the base model succeeded, but it took at least a couple tries.\n",
    "\n",
    "When we steer the base model by features, does the number 5.05 decline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e87d281-c38d-4ced-9d52-5af91f193ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average samples to solve these 22 tasks: 5.05 (111 total runs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HumanEval/5': 6,\n",
       " 'HumanEval/26': 5,\n",
       " 'HumanEval/33': 10,\n",
       " 'HumanEval/69': 3,\n",
       " 'HumanEval/76': 3,\n",
       " 'HumanEval/77': 3,\n",
       " 'HumanEval/83': 5,\n",
       " 'HumanEval/87': 3,\n",
       " 'HumanEval/88': 5,\n",
       " 'HumanEval/89': 5,\n",
       " 'HumanEval/91': 10,\n",
       " 'HumanEval/99': 3,\n",
       " 'HumanEval/100': 3,\n",
       " 'HumanEval/101': 5,\n",
       " 'HumanEval/106': 4,\n",
       " 'HumanEval/109': 3,\n",
       " 'HumanEval/119': 7,\n",
       " 'HumanEval/126': 5,\n",
       " 'HumanEval/140': 4,\n",
       " 'HumanEval/142': 6,\n",
       " 'HumanEval/151': 5,\n",
       " 'HumanEval/160': 8}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenging_tasks = {task_id: n_samples for task_id, n_samples in n_samples_by_task_id.items() if n_samples and n_samples > 2}\n",
    "print(f\"Average samples to solve these {len(challenging_tasks)} tasks: {np.mean(list(challenging_tasks.values())):.2f} ({sum(list(challenging_tasks.values()))} total runs)\")\n",
    "challenging_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf41d4b6-5231-4f77-b34b-d74cdc84d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (102,)\n",
      "2 (25,)\n",
      "None (15,)\n"
     ]
    }
   ],
   "source": [
    "xs = np.array(list(n_samples_by_task_id.values()))\n",
    "for x in [1, 2, None]:\n",
    "    print(x, xs[xs == x].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95c754-0feb-4ba5-af82-83cc2189c6a3",
   "metadata": {},
   "source": [
    "### Computational savings\n",
    "The base model required 111 rollouts total to solve these 22 tasks.\n",
    "\n",
    "The pass@10 metric required 1640 rollouts to compute.\n",
    "\n",
    "This gives us the signal we're looking for with 6.7% of the compute - that's about 15x faster, or ~30 seconds instead of 8 minutes on an L40S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702dfcc-0acd-4273-9411-0339973e8aca",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Further optimization: fit a Gaussian to n_samples, stop testing additional tasks once we're confident enough\n",
    "\n",
    "Could provide another OOM of sample efficiency.\n",
    "\n",
    "Probably not worth the additional complexity, but I didn't realize that until after I'd gotten this working... maybe I'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98833dda-a1c5-41f3-8444-6879a9d04d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.705882352941174, 3.842380205681849)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "MAX_SAMPLES = 10\n",
    "\n",
    "def fit_gaussian(xs: List[int], fill_value = MAX_SAMPLES) -> Tuple[float, float]:\n",
    "    gmm = GaussianMixture(n_components=1, covariance_type='spherical')\n",
    "    xs = np.array(xs).reshape(-1, 1)\n",
    "    xs[xs == None] = fill_value  # But really, this could be much larger than fill_value. How to handle this?\n",
    "    gmm.fit(xs)\n",
    "    mean = gmm.means_[0][0]\n",
    "    var = np.sqrt(gmm.covariances_[0])\n",
    "    return mean.item(), np.sqrt(var).item()\n",
    "\n",
    "fit_gaussian([10, 16, 8, 5, 11, 15, 15, 15, 15, 13, 10, 15, 19, 5, 2, 60, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b01d7269-7f3c-402c-8d15-772c8c652ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean samples for pass: 1.73 (std: 1.19)\n",
      "mean samples for pass: 1.56 (std: 1.11)\n",
      "mean samples for pass: 1.52 (std: 1.05)\n",
      "mean samples for pass: 1.58 (std: 1.10)\n",
      "mean samples for pass: 1.48 (std: 1.06)\n",
      "mean samples for pass: 1.94 (std: 1.49)\n",
      "mean samples for pass: 1.85 (std: 1.45)\n",
      "mean samples for pass: 1.76 (std: 1.41)\n",
      "mean samples for pass: 1.69 (std: 1.38)\n",
      "mean samples for pass: 1.62 (std: 1.36)\n",
      "mean samples for pass: 1.57 (std: 1.33)\n",
      "mean samples for pass: 1.55 (std: 1.31)\n",
      "mean samples for pass: 1.55 (std: 1.29)\n",
      "mean samples for pass: 1.64 (std: 1.37)\n",
      "mean samples for pass: 1.67 (std: 1.35)\n",
      "mean samples for pass: 1.69 (std: 1.35)\n",
      "mean samples for pass: 1.77 (std: 1.36)\n",
      "mean samples for pass: 1.82 (std: 1.41)\n",
      "mean samples for pass: 1.82 (std: 1.40)\n",
      "mean samples for pass: 1.91 (std: 1.44)\n",
      "mean samples for pass: 2.00 (std: 1.48)\n",
      "mean samples for pass: 2.04 (std: 1.51)\n",
      "mean samples for pass: 2.20 (std: 1.58)\n",
      "mean samples for pass: 2.17 (std: 1.56)\n",
      "mean samples for pass: 2.37 (std: 1.64)\n",
      "mean samples for pass: 2.46 (std: 1.67)\n",
      "mean samples for pass: 2.50 (std: 1.68)\n",
      "mean samples for pass: 2.55 (std: 1.69)\n",
      "mean samples for pass: 2.50 (std: 1.68)\n",
      "mean samples for pass: 2.49 (std: 1.67)\n",
      "mean samples for pass: 2.48 (std: 1.67)\n",
      "pass@10: 0.909, rollouts averted: 1227 (74.8%)\n"
     ]
    }
   ],
   "source": [
    "fails = 0\n",
    "n_samples_by_task_id = {}\n",
    "rollouts_averted = 0\n",
    "\n",
    "MIN_TASKS = 10\n",
    "\n",
    "for i, (task_id, samples) in enumerate(results['eval'].items()):\n",
    "    # iterate through our samples, looking for the first one to pass\n",
    "    for n, attempt in enumerate(samples):\n",
    "        n += 1  # make n_samples 1-indexed\n",
    "        if attempt['base_status'] == 'pass':\n",
    "            n_samples_by_task_id[task_id] = n\n",
    "            rollouts_averted += (MAX_SAMPLES - n)\n",
    "            break\n",
    "    else:  # no successful samples\n",
    "        fails += 1\n",
    "        assert n == MAX_SAMPLES\n",
    "        n_samples_by_task_id[task_id] = None\n",
    "    \n",
    "    if len(n_samples_by_task_id) > MIN_TASKS:\n",
    "        mean, stddev = fit_gaussian(list(n_samples_by_task_id.values()))\n",
    "        if i % 5 == 0:\n",
    "            print(f\"mean samples for pass: {mean:.2f} (std: {stddev:.2f})\")\n",
    "\n",
    "n_tasks = len(results['eval'])\n",
    "n_runs = n_tasks * MAX_SAMPLES  # assumes pass@k, which generates MAX_SAMPLES for each problem\n",
    "print(f\"pass@10: {1-fails/n_tasks:.3f}, rollouts averted: {rollouts_averted} ({rollouts_averted/n_runs*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da70f3-e09a-4e77-9a34-1c31c3d4f0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
